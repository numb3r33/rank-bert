{"cells":[{"cell_type":"markdown","metadata":{"id":"wpZYNICAt7jF"},"source":["# Model Architecture\n","\n","> Implementation of BERT model variants for rank manipulation experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_OiVYuqt7jG"},"outputs":[],"source":["#| default_exp models.base_models"]},{"cell_type":"code","source":["#| hide\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/rank-bert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niAcwY2duCpB","executionInfo":{"status":"ok","timestamp":1742275779336,"user_tz":-330,"elapsed":20227,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}},"outputId":"bd3b9aa8-bff0-432e-a441-2db34dda2208"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/rank-bert\n"]}]},{"cell_type":"code","source":["#| hide\n","!pip install -q nbdev datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBXMnK5Pujn7","executionInfo":{"status":"ok","timestamp":1742275789576,"user_tz":-330,"elapsed":10240,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}},"outputId":"c653a074-2c86-4426-caf6-9b62f564e6df"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"emlWZjqXt7jH","executionInfo":{"status":"ok","timestamp":1742275790385,"user_tz":-330,"elapsed":787,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| hide\n","from nbdev.showdoc import *"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"i794_uqft7jH","executionInfo":{"status":"ok","timestamp":1742275828950,"user_tz":-330,"elapsed":38561,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from transformers import AutoConfig, AutoModelForSequenceClassification, BertConfig, BertForSequenceClassification\n","from fastai.text.all import *"]},{"cell_type":"markdown","metadata":{"id":"d2lvkW0Et7jH"},"source":["## BERT Model Variants\n","\n","We'll implement several BERT model variants for our experiments. According to our technical specification, we need:\n","\n","1. **BERT-tiny**: A very small BERT model with 2 layers, 128 hidden size, and 2 attention heads\n","2. **BERT-mini**: 4 layers, 256 hidden size, 4 attention heads\n","3. **BERT-small**: 4 layers, 512 hidden size, 8 attention heads\n","\n","We'll use the HuggingFace Transformers library to initialize these models."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"JCWvusvgt7jH","executionInfo":{"status":"ok","timestamp":1742275829058,"user_tz":-330,"elapsed":96,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","# GLUE task constants\n","GLUE_NUM_LABELS = {\n","    'sst2': 2,\n","    'mrpc': 2,\n","    'rte': 2\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"hyl9M6eJt7jI","executionInfo":{"status":"ok","timestamp":1742275829071,"user_tz":-330,"elapsed":100,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","# Model configuration constants\n","BERT_CONFIGS = {\n","    'prajjwal1/bert-tiny': {\n","        'hidden_size': 128,\n","        'num_hidden_layers': 2,\n","        'num_attention_heads': 2,\n","        'intermediate_size': 512\n","    },\n","    'prajjwal1/bert-mini': {\n","        'hidden_size': 256,\n","        'num_hidden_layers': 4,\n","        'num_attention_heads': 4,\n","        'intermediate_size': 1024\n","    },\n","    'prajjwal1/bert-small': {\n","        'hidden_size': 512,\n","        'num_hidden_layers': 4,\n","        'num_attention_heads': 8,\n","        'intermediate_size': 2048\n","    }\n","}"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"bEfbIEX8t7jI","executionInfo":{"status":"ok","timestamp":1742275829072,"user_tz":-330,"elapsed":46,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","def get_pretrained_model(model_name, task_name, num_labels=None):\n","    \"\"\"\n","    Initialize a pretrained model for a specific task.\n","\n","    Args:\n","        model_name (str): HuggingFace model name or path (e.g., 'prajjwal1/bert-tiny', 'bert-mini')\n","        task_name (str): GLUE task name ('sst2', 'mrpc', 'rte')\n","        num_labels (int, optional): Number of output labels\n","\n","    Returns:\n","        PreTrainedModel: Initialized model\n","    \"\"\"\n","    # Get the number of labels for the task\n","    num_labels = num_labels or GLUE_NUM_LABELS.get(task_name, 2)\n","\n","    # Check if the model name is a known configuration or a HuggingFace model\n","    if model_name in BERT_CONFIGS:\n","        # Create a new model with the specified configuration\n","        config = BertConfig(\n","            **BERT_CONFIGS[model_name],\n","            num_labels=num_labels,\n","            hidden_dropout_prob=0.1,\n","            attention_probs_dropout_prob=0.1\n","        )\n","        model = BertForSequenceClassification(config)\n","    else:\n","        # Load a pretrained model from HuggingFace\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            model_name,\n","            num_labels=num_labels\n","        )\n","\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"z4zqumTLt7jI","executionInfo":{"status":"ok","timestamp":1742275834663,"user_tz":-330,"elapsed":42,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","def count_parameters(model):\n","    \"\"\"\n","    Count number of trainable parameters in a model.\n","\n","    Args:\n","        model: PyTorch model\n","\n","    Returns:\n","        Number of trainable parameters\n","    \"\"\"\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Nn8bGI0-t7jI","executionInfo":{"status":"ok","timestamp":1742275847365,"user_tz":-330,"elapsed":24,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","class BertWrapper(Module):\n","    \"\"\"\n","    Wrapper around BERT model for fastai integration.\n","    Handles input formatting and output processing.\n","\n","    This class serves as a base for rank-constrained models,\n","    making it easier to modify and monitor model behavior.\n","    \"\"\"\n","\n","    def __init__(self, model):\n","        \"\"\"\n","        Initialize the BERT wrapper.\n","\n","        Args:\n","            model: Pretrained BERT model\n","        \"\"\"\n","        self.model = model\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass through the model.\n","\n","        Args:\n","            x: Dictionary of inputs from tokenizer\n","\n","        Returns:\n","            Model outputs\n","        \"\"\"\n","        # Handle either dict or tuple input\n","        if isinstance(x, tuple):\n","            x = x[0]\n","\n","        # Extract and ensure inputs are on the correct device\n","        input_ids = x['input_ids']\n","        attention_mask = x['attention_mask']\n","        token_type_ids = x.get('token_type_ids', None)\n","\n","        # Forward pass through BERT\n","        if token_type_ids is not None:\n","            outputs = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids\n","            )\n","        else:\n","            outputs = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","\n","        return outputs.logits"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jIV5Ou_ht7jJ","executionInfo":{"status":"ok","timestamp":1742275849674,"user_tz":-330,"elapsed":3,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","def get_wrapped_model(model_name, task_name, num_labels=None):\n","    \"\"\"\n","    Get a wrapped BERT model for fastai integration.\n","\n","    Args:\n","        model_name (str): HuggingFace model name or path\n","        task_name (str): GLUE task name\n","        num_labels (int, optional): Number of output labels\n","\n","    Returns:\n","        BertWrapper: Wrapped model\n","    \"\"\"\n","    model = get_pretrained_model(model_name, task_name, num_labels)\n","    return BertWrapper(model)"]},{"cell_type":"markdown","metadata":{"id":"nMY_r52zt7jJ"},"source":["## Example Usage\n","\n","Here's how we can create different BERT model variants and check their parameter counts."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5vSc-o9t7jJ","executionInfo":{"status":"ok","timestamp":1742275875802,"user_tz":-330,"elapsed":1050,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}},"outputId":"fc5a4ff0-76fb-43fa-b2b5-9a3247ae9750"},"outputs":[{"output_type":"stream","name":"stdout","text":["prajjwal1/bert-tiny: 4,386,178 parameters\n","prajjwal1/bert-mini: 11,171,074 parameters\n","prajjwal1/bert-small: 28,764,674 parameters\n"]}],"source":["# Example: Create and compare different BERT variants\n","models = {}\n","for variant in ['prajjwal1/bert-tiny', 'prajjwal1/bert-mini', 'prajjwal1/bert-small']:\n","    models[variant] = get_pretrained_model(variant, 'sst2')\n","\n","# Compare parameter counts\n","for name, model in models.items():\n","    print(f\"{name}: {count_parameters(model):,} parameters\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYPBqCt9t7jJ","executionInfo":{"status":"ok","timestamp":1742275884936,"user_tz":-330,"elapsed":292,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}},"outputId":"90719a60-9649-4c66-e25e-9d9dde31e6d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wrapped model has 4,386,178 parameters\n"]}],"source":["# Example: Create a wrapped model for fastai\n","wrapped_model = get_wrapped_model('prajjwal1/bert-tiny', 'mrpc')\n","print(f\"Wrapped model has {count_parameters(wrapped_model):,} parameters\")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"I_GWRN_8t7jJ","executionInfo":{"status":"ok","timestamp":1742275898103,"user_tz":-330,"elapsed":6127,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| hide\n","import nbdev; nbdev.nbdev_export()"]}],"metadata":{"kernelspec":{"display_name":"python3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}