{"cells":[{"cell_type":"markdown","metadata":{"id":"1HlHDWCTmRnG"},"source":["# Data\n","\n","> Utilities for loading and processing GLUE datasets for BERT rank experiments"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"DWPu4DWZmRnG","executionInfo":{"status":"ok","timestamp":1742271133550,"user_tz":-330,"elapsed":12,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| default_exp data.load_data"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21945,"status":"ok","timestamp":1742274786479,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"},"user_tz":-330},"id":"7c0RgjDtZcj-","outputId":"68300983-00ab-4d68-fc18-40c8795acf7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/rank-bert\n"]}],"source":["#| hide\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/rank-bert"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":28529,"status":"ok","timestamp":1742274815015,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"},"user_tz":-330},"id":"pvkSfSGIYlJp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f764e0b-beea-4382-efa0-7c53ee412ae5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["#| hide\n","!pip install -q nbdev datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"thChQcpEmRnH","executionInfo":{"status":"ok","timestamp":1742271142693,"user_tz":-330,"elapsed":824,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| hide\n","from nbdev.showdoc import *"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1742271191628,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"},"user_tz":-330},"id":"QV2vlQldmRnH"},"outputs":[],"source":["#| export\n","import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","import copy\n","\n","from fastai.text.all import *\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","from rank_bert.data.transforms import TransTensorText, Undict, TokBatchTransform\n","from torch.utils.data._utils.collate import default_collate"]},{"cell_type":"markdown","metadata":{"id":"uASikSJRmRnH"},"source":["## GLUE Dataset Management\n","\n","We need to handle loading and processing GLUE datasets for our experiments. We'll create a `GLUEDataManager` class that manages the loading and preprocessing of GLUE datasets, specifically SST-2 (sentiment analysis), MRPC (paraphrase detection), and RTE (textual entailment)."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1742271196261,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"},"user_tz":-330},"id":"XR8hLCTCmRnH"},"outputs":[],"source":["#| export\n","class F1Score:\n","    \"Custom F1 Score metric for fastai\"\n","    def __init__(self, average='binary'):\n","        self.average = average\n","\n","    def __call__(self, preds, targets):\n","        preds = torch.argmax(preds, dim=1)\n","        return f1_score(targets.cpu().numpy(), preds.cpu().numpy(), average=self.average)\n","\n","    def __repr__(self):\n","        return f\"F1Score(average={self.average})\""]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1742271196540,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"},"user_tz":-330},"id":"Ujbe7WLTmRnI"},"outputs":[],"source":["#| export\n","def accuracy(preds, targets):\n","    \"Accuracy metric for fastai\"\n","    preds = torch.argmax(preds, dim=1)\n","    return (preds == targets).float().mean()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":53,"status":"ok","timestamp":1742271196968,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"},"user_tz":-330},"id":"6-CSqbyUmRnI"},"outputs":[],"source":["#| export\n","class GLUEDataManager:\n","    \"\"\"Manager for GLUE dataset loading and processing.\"\"\"\n","\n","    def __init__(self, task_name, model_name, max_length=512, bs=32, val_bs=None, cache_dir=None):\n","        self.task_name = task_name.lower()\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.bs = bs\n","        self.val_bs = val_bs or 2*bs\n","        self.cache_dir = cache_dir\n","\n","        if self.task_name not in ['sst2', 'mrpc', 'rte']:\n","            raise ValueError(f\"Task {self.task_name} not supported. Use one of: sst2, mrpc, rte\")\n","\n","        self.text_fields = {\n","            'sst2': ['sentence', None],\n","            'mrpc': ['sentence1', 'sentence2'],\n","            'rte': ['sentence1', 'sentence2']\n","        }\n","\n","        self.metrics = {\n","            'sst2': [accuracy],\n","            'mrpc': [F1Score(), accuracy],\n","            'rte': [accuracy]\n","        }\n","\n","        self.num_labels = {\n","            'sst2': 2,\n","            'mrpc': 2,\n","            'rte': 2\n","        }\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    def load_datasets(self, custom_datasets=None, max_samples=None):\n","        print(f\"Loading datasets for {self.task_name}...\")\n","\n","        if custom_datasets is not None:\n","            datasets = custom_datasets\n","        else:\n","            datasets = load_dataset('glue', self.task_name, cache_dir=self.cache_dir)\n","\n","        if max_samples is not None:\n","            for split in datasets.keys():\n","                if split != 'test':\n","                    datasets[split] = datasets[split].select(range(min(max_samples, len(datasets[split]))))\n","\n","        print(f\"Dataset sizes: {', '.join([f'{k}: {len(v)}' for k, v in datasets.items()])}\")\n","        self.datasets = datasets\n","        return datasets\n","\n","    def _prepare_fastai_data(self, datasets):\n","        text_field1, text_field2 = self.text_fields[self.task_name]\n","\n","        train_texts, train_labels = [], []\n","        for item in datasets['train']:\n","            if text_field2 is not None:\n","                train_texts.append((item[text_field1], item[text_field2]))\n","            else:\n","                train_texts.append(item[text_field1])\n","            train_labels.append(item['label'])\n","\n","        val_texts, val_labels = [], []\n","        for item in datasets['validation']:\n","            if text_field2 is not None:\n","                val_texts.append((item[text_field1], item[text_field2]))\n","            else:\n","                val_texts.append(item[text_field1])\n","            val_labels.append(item['label'])\n","\n","        return train_texts, train_labels, val_texts, val_labels\n","\n","    def create_dataloaders(self, custom_datasets=None, max_samples=None):\n","        if not hasattr(self, 'datasets') or custom_datasets is not None:\n","            self.load_datasets(custom_datasets, max_samples)\n","\n","        train_texts, train_labels, val_texts, val_labels = self._prepare_fastai_data(self.datasets)\n","\n","        # Combine train and validation data for DataBlock\n","        all_texts = train_texts + val_texts\n","        all_labels = train_labels + val_labels\n","        df = pd.DataFrame({'text': all_texts, 'label': all_labels})\n","\n","        # Calculate sequence lengths for sorting (optional, but helpful for efficiency)\n","        train_lens = [len(str(t)) for t in train_texts]\n","        val_lens = [len(str(t)) for t in val_texts]\n","\n","        # Use the reference code pattern for the DataBlock setup\n","        dls_kwargs = {\n","            'before_batch': TokBatchTransform(\n","                pretrained_model_name=self.model_name,\n","                max_length=self.max_length,\n","                padding='max_length',\n","                truncation=True\n","            ),\n","            'create_batch': fa_convert  # Use fastai's standard batch creation\n","        }\n","\n","        # Define the text block with the same structure as reference\n","        text_block = TransformBlock(\n","            dl_type=SortedDL,  # Use SortedDL to enable length-based sorting\n","            dls_kwargs=dls_kwargs,\n","            batch_tfms=Undict()  # Add Undict for decoding\n","        )\n","\n","        # Create DataBlock\n","        glue_block = DataBlock(\n","            blocks=[text_block, CategoryBlock()],\n","            get_x=ColReader('text'),\n","            get_y=ColReader('label'),\n","            splitter=IndexSplitter(range(len(train_texts), len(train_texts) + len(val_labels)))\n","        )\n","\n","        # Create DataLoaders with length-based resources for efficiency\n","        dl_kwargs = [{'res': train_lens}, {'val_res': val_lens}]\n","        dls = glue_block.dataloaders(\n","            df,\n","            bs=self.bs,\n","            val_bs=self.val_bs,\n","            dl_kwargs=dl_kwargs\n","        )\n","\n","        self.dls = dls\n","        return dls\n","\n","    def create_test_dataloader(self, test_data=None):\n","        if not hasattr(self, 'dls'):\n","            raise ValueError(\"You must create training DataLoaders first by calling create_dataloaders()\")\n","\n","        test_data = test_data or self.datasets.get('test')\n","        if test_data is None:\n","            raise ValueError(\"No test data available.\")\n","\n","        text_field1, text_field2 = self.text_fields[self.task_name]\n","        test_texts = []\n","        for item in test_data:\n","            if text_field2 is not None:\n","                test_texts.append((item[text_field1], item[text_field2]))\n","            else:\n","                test_texts.append(item[text_field1])\n","\n","        test_df = pd.DataFrame({'text': test_texts, 'label': [0] * len(test_texts)})\n","        test_dl = self.dls.test_dl(test_df)\n","        return test_dl\n","\n","class TextGetter(ItemTransform):\n","    \"\"\"ItemTransform for getting text fields from a sample\"\"\"\n","    def __init__(self, s1='text', s2=None):\n","        self.s1, self.s2 = s1, s2\n","    def encodes(self, sample):\n","        if self.s2 is None: return sample[self.s1]\n","        else: return sample[self.s1], sample[self.s2]"]},{"cell_type":"markdown","metadata":{"id":"jhTq8ofPmRnI"},"source":["## Example Usage\n","\n","Here's an example of how to use the `GLUEDataManager` to load and prepare a dataset for training."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5995,"status":"ok","timestamp":1742271209127,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"},"user_tz":-330},"id":"iIMQgG4ImRnI","outputId":"748cf7d6-e354-4205-f1db-ddb4b36b43db"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loading datasets for sst2...\n","Dataset sizes: train: 100, validation: 100, test: 1821\n"]}],"source":["# # Example usage (commented out for export)\n","\n","# data_manager = GLUEDataManager(\n","#     task_name='sst2',\n","#     model_name='prajjwal1/bert-tiny',\n","#     max_length=128,\n","#     bs=16\n","# )\n","\n","# # Load a small subset for testing\n","# dls = data_manager.create_dataloaders(max_samples=100)\n","# dls.show_batch(max_n=2)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Kys9QU6dmRnI","executionInfo":{"status":"ok","timestamp":1742274833855,"user_tz":-330,"elapsed":226,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| hide\n","import nbdev; nbdev.nbdev_export()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}