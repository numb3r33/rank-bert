{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDRvg5vIurOnVv8U+8AqrT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#| default_exp data.transforms"],"metadata":{"id":"1lYZoZ_1sJc3","executionInfo":{"status":"ok","timestamp":1742271102392,"user_tz":-330,"elapsed":12,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#| hide\n","!pip install -q nbdev"],"metadata":{"id":"3fXobcqssv5i","executionInfo":{"status":"ok","timestamp":1742271106618,"user_tz":-330,"elapsed":4225,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#| hide\n","from nbdev.showdoc import *"],"metadata":{"id":"eoaiDQAzsOfZ","executionInfo":{"status":"ok","timestamp":1742271107375,"user_tz":-330,"elapsed":749,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#| hide\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/rank-bert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hq4EuP_TcC2w","executionInfo":{"status":"ok","timestamp":1742271109248,"user_tz":-330,"elapsed":1869,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}},"outputId":"50134632-0565-4f52-a22b-ab70a8414b10"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/rank-bert\n"]}]},{"cell_type":"code","source":["#| export\n","import torch\n","from fastai.text.all import *\n","from transformers import AutoTokenizer\n","from torch.utils.data._utils.collate import default_collate"],"metadata":{"id":"qex4zNqOsSDQ","executionInfo":{"status":"ok","timestamp":1742271126075,"user_tz":-330,"elapsed":16820,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["\n","## Batch transformation utilities for BERT rank experiments.\n","\n","This module provides the necessary transform classes for tokenizing batches of text\n","for BERT models, handling both single-text and text-pair inputs.\n"],"metadata":{"id":"0IVigyEBsepL"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"X9CbNjvysCfS","executionInfo":{"status":"ok","timestamp":1742271126161,"user_tz":-330,"elapsed":60,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"outputs":[],"source":["#| export\n","class TransTensorText(TensorBase): pass\n","\n","class Undict(Transform):\n","    \"\"\"Transform to convert tokenizer output dict to TransTensorText for decoding\"\"\"\n","    def decodes(self, x:dict):\n","        if 'input_ids' in x:\n","            res = TransTensorText(x['input_ids'])\n","            return res\n","        return x\n","\n","class TokBatchTransform(Transform):\n","    \"\"\"\n","    Tokenizes texts in batches using pretrained HuggingFace tokenizer.\n","    Following the reference implementation pattern.\n","    \"\"\"\n","    def __init__(self, pretrained_model_name=None, tokenizer_cls=AutoTokenizer,\n","                 config=None, tokenizer=None, with_labels=False,\n","                 padding=True, truncation=True, max_length=None, **kwargs):\n","        if tokenizer is None:\n","            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name, config=config)\n","        self.tokenizer = tokenizer\n","        self.kwargs = kwargs\n","        self._two_texts = False\n","        store_attr()\n","\n","    def encodes(self, batch):\n","        # Handle initialization case - if batch is None or empty\n","        if batch is None or len(batch) == 0:\n","            # Return a dummy structure for initialization\n","            dummy = {'input_ids': torch.zeros(1, 10, dtype=torch.long),\n","                    'attention_mask': torch.zeros(1, 10, dtype=torch.long)}\n","\n","            if 'token_type_ids' in self.tokenizer.model_input_names:\n","                dummy['token_type_ids'] = torch.zeros(1, 10, dtype=torch.long)\n","\n","            return (dummy, torch.zeros(1, dtype=torch.long))\n","\n","        # Check if first item is a tuple (text pair)\n","        if is_listy(batch[0][0]):\n","            self._two_texts = True\n","            texts = ([s[0][0] for s in batch], [s[0][1] for s in batch])\n","        else:\n","            texts = ([s[0] for s in batch],)\n","\n","        # Tokenize texts\n","        inputs = self.tokenizer(*texts,\n","                              add_special_tokens=True,\n","                              padding=self.padding,\n","                              truncation=self.truncation,\n","                              max_length=self.max_length,\n","                              return_tensors='pt',\n","                              **self.kwargs)\n","\n","        # Collate labels\n","        labels = default_collate([s[1:] for s in batch])\n","\n","        # Return structure depends on with_labels flag\n","        if self.with_labels:\n","            inputs['labels'] = labels[0]\n","            return (inputs, )\n","        else:\n","            return (inputs, ) + tuple(labels)\n","\n","    def decodes(self, x):\n","        if isinstance(x, TransTensorText):\n","            if self._two_texts:\n","                x1, x2 = split_by_sep(x, self.tokenizer.sep_token_id)\n","                return (TitledStr(self.tokenizer.decode(x1.cpu(), skip_special_tokens=True)),\n","                        TitledStr(self.tokenizer.decode(x2.cpu(), skip_special_tokens=True)))\n","            return TitledStr(self.tokenizer.decode(x.cpu(), skip_special_tokens=True))\n","        return x"]},{"cell_type":"code","source":["import nbdev; nbdev.nbdev_export()"],"metadata":{"id":"n8dbkDhFsnqd","executionInfo":{"status":"ok","timestamp":1742271128054,"user_tz":-330,"elapsed":1865,"user":{"displayName":"Abhishek Sharma","userId":"11883431818886671775"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IlRiDAQYcRAq"},"execution_count":null,"outputs":[]}]}